# 设备维修智能问答系统 (Equipment Maintenance Intelligent Q&A System)

## 1. 项目概述

本项目是一个基于 **RAG (Retrieval-Augmented Generation)** 技术的智能问答系统前端与服务层实现，专为设备维修场景设计。系统结合了 **Next.js** 构建的现代化前端界面与 **Python (FastAPI + CLIP)** 构建的检索服务，能够根据用户输入的维修问题，实时检索相关技术文档，并利用大语言模型 (LLM) 生成结构化的中文维修指南。

### 核心价值
- **智能化检索**：利用 CLIP 模型对多模态/文本语义进行深度理解，精准匹配维修手册中的相关段落。
- **结构化输出**：通过精心设计的 Prompt Engineering，强制模型输出包含“所需工具”和“详细步骤”的标准格式，规范维修流程。
- **多会话管理**：支持同时进行多个独立的维修咨询任务，上下文互不干扰。
- **可视化溯源**：提供检索来源的原文查看功能，确保回答的可信度与可追溯性。

---

## 2. 技术栈与架构

### 2.1 前端架构 (Next.js)
前端采用 **Next.js 15 (App Router)** 框架，结合 **TypeScript** 进行类型安全开发。

*   **框架**: Next.js 15, React 19
*   **语言**: TypeScript
*   **样式**: Tailwind CSS v4 (原子化 CSS), Lucide React (图标库)
*   **状态管理**: React Hooks (`useState`, `useEffect`, `useMemo`, `useRef`)
*   **HTTP 客户端**: 原生 `fetch` API

### 2.2 后端服务架构
系统包含两个主要的后端服务组件：

1.  **Next.js API Routes (BFF 层)**:
    *   作为前端与底层服务的中间层。
    *   负责请求转发、Prompt 拼接、错误处理。
    *   使用 `openai` Node.js SDK 连接大模型接口。

2.  **RAG 检索服务 (Python)**:
    *   **框架**: FastAPI, Uvicorn
    *   **核心库**: PyTorch, CLIP (OpenAI), NumPy
    *   **功能**: 加载向量数据库 (`.npz`) 和原始文档 (`.json`)，执行向量相似度计算。

3.  **大模型服务 (LLM)**:
    *   **模型**: Qwen2.5-14B-Instruct (通义千问)
    *   **部署**: vLLM (提供 OpenAI 兼容接口)

---

## 3. 目录结构说明

```text
/mnt/bit/liyuanxi/projects/zhongche/rag-chat-next/
├── app/                        # Next.js App Router 主目录
│   ├── api/                    # 后端 API 路由
│   │   └── chat/
│   │       └── route.ts        # 核心对话接口：处理 RAG 流程与 LLM 调用
│   ├── globals.css             # 全局样式 (Tailwind 指令)
│   ├── layout.tsx              # 根布局文件
│   └── page.tsx                # 主页面逻辑 (状态管理、UI 组装)
├── components/                 # React UI 组件
│   ├── ChatComposer.tsx        # 底部输入框组件
│   ├── ChatMessage.tsx         # 单条消息气泡组件
│   ├── Header.tsx              # 顶部导航栏
│   ├── RagPanel.tsx            # 右侧内容来源面板 (含弹窗逻辑)
│   └── Sidebar.tsx             # 左侧会话列表侧边栏
├── lib/                        # 工具库
│   ├── types.ts                # TypeScript 类型定义 (Message, Session, RagDoc)
│   └── utils.ts                # 通用辅助函数 (uid 生成, cn 类名合并)
├── model_api.py                # (参考) 原始模型调用示例脚本
├── rag_service.py              # Python RAG 检索微服务入口
├── package.json                # 项目依赖配置
├── tsconfig.json               # TypeScript 配置
└── next.config.ts              # Next.js 配置文件
```

---

## 4. 功能模块详解

### 4.1 用户界面 (UI) 设计

界面采用经典的 **三栏布局**，响应式设计适配不同屏幕尺寸。

#### 4.1.1 顶部导航栏 (`Header.tsx`)
*   **样式**: 固定在顶部，半透明磨砂玻璃效果 (`backdrop-blur`)。
*   **内容**: 左侧展示系统名称“设备维修智能问答系统”及图标。
*   **作用**: 提供全局统一的品牌标识。

#### 4.1.2 左侧侧边栏 (`Sidebar.tsx`)
*   **功能**: **多会话管理**。
*   **交互**:
    *   **新建会话**: 点击“新建”按钮，创建一个空白的聊天上下文。
    *   **切换会话**: 点击列表中的条目，无缝切换到对应的历史对话。
    *   **自动命名**: 列表项展示会话标题，标题会根据用户的第一条提问自动生成，且支持长文本省略显示 (`truncate`)。
*   **样式**: 灰色背景，选中项高亮显示，支持滚动。

#### 4.1.3 中间对话区 (`page.tsx`, `ChatMessage.tsx`, `ChatComposer.tsx`)
*   **消息流**:
    *   显示用户与助手的对话记录。
    *   **自动滚动**: 新消息到达时，页面自动平滑滚动到底部 (`scrollIntoView`)。
    *   **加载状态**: 助手回复时显示 "Assistant is typing..." 动画。
*   **输入框**:
    *   固定在底部，不随消息滚动。
    *   包含多行文本域 (`textarea`) 和“发送”按钮。
    *   发送按钮加宽设计，防止文字换行。

#### 4.1.4 右侧内容来源面板 (`RagPanel.tsx`)
*   **功能**: 展示 RAG 检索到的参考文档。
*   **交互**:
    *   **卡片展示**: 每个参考文档以卡片形式展示标题和摘要。
    *   **详情弹窗**: 点击卡片，弹出一个模态窗口 (Modal)，显示文档的**全部内容**。
    *   **弹窗交互**: 弹窗包含遮罩层，点击遮罩或右上角关闭按钮即可退出；弹窗头部固定，内容区域独立滚动。
*   **数据流**: 每次对话产生的检索结果会**追加**到列表中，形成完整的知识溯源链。

### 4.2 核心业务流程 (Code Logic)

#### 4.2.1 前端状态管理 (`app/page.tsx`)
使用 `useState` 维护核心状态 `sessions` (会话列表)，结构如下：
```typescript
interface Session {
    id: string;
    title: string;          // 会话标题
    messages: Message[];    // 消息历史
    ragResults: RagDoc[];   // 关联的 RAG 检索结果
    createdAt: number;
}
```
*   **乐观更新 (Optimistic Update)**: 用户点击发送后，立即在界面显示用户消息，无需等待服务器响应。
*   **会话隔离**: 所有的消息和 RAG 结果都绑定在 `activeId` 对应的 Session 对象上，切换 ID 即切换所有视图数据。

#### 4.2.2 后端 RAG 流程 (`app/api/chat/route.ts`)
这是系统的“大脑”，负责协调检索与生成。

1.  **接收请求**: 获取前端发送的完整消息历史。
2.  **调用检索服务**:
    *   提取用户最新的一条问题。
    *   向 `http://127.0.0.1:8001/retrieve` 发起 POST 请求。
3.  **构建 Prompt**:
    *   如果检索成功，将文档内容注入 Prompt：
        > "{用户问题} Please answer based on the reference document... Reference Document: {文档内容}"
    *   **格式约束**: 在 Prompt 中硬编码了输出格式要求（工具列表 + 步骤列表）。
4.  **调用 LLM**:
    *   使用 `openai` SDK 连接本地 vLLM 接口。
    *   发送包含 System Prompt 和 User Prompt 的请求。
5.  **返回结果**: 将 LLM 的回复文本和 RAG 检索到的文档元数据一同返回给前端。

#### 4.2.3 向量检索服务 (`rag_service.py`)
这是系统的“记忆”，负责从知识库中找到答案。

1.  **初始化**:
    *   启动时加载 **CLIP ViT-B/32** 模型到 GPU (CUDA)。
    *   加载预计算好的向量数据库 (`all_embeddings.npz`)。
    *   加载原始文本数据 (`train_data_all.json`)。
2.  **检索逻辑 (`/retrieve`)**:
    *   接收查询文本。
    *   使用 CLIP `tokenize` 和 `encode_text` 将文本转换为向量。
    *   执行向量归一化。
    *   计算查询向量与数据库向量的 **余弦相似度** (`torch.einsum`)。
    *   找到相似度最高的索引 (`argmax`)。
3.  **返回数据**: 返回对应的原始文档内容 (`output` 字段) 及标题。

---

## 5. 部署与运行指南

### 5.1 环境准备
*   **Node.js**: v18+
*   **Python**: 3.8+ (建议使用 Conda)
*   **GPU**: 建议配备 NVIDIA 显卡以加速 CLIP 和 LLM 推理。

### 5.2 启动步骤

需要开启三个终端窗口分别运行以下服务：

#### 步骤 1: 启动大模型 (vLLM)
```bash
# 假设已安装 vllm
python -m vllm.entrypoints.openai.api_server \
    --model /mnt/bit/wxc/projects/zhongche-llm/Qwen2.5-14B-Instruct \
    --served-model-name /mnt/bit/wxc/projects/zhongche-llm/Qwen2.5-14B-Instruct \
    --trust-remote-code \
    --port 8000
```

#### 步骤 2: 启动 RAG 检索服务
```bash
# 进入项目目录
cd /mnt/bit/liyuanxi/projects/zhongche/rag-chat-next

# 安装依赖
pip install fastapi uvicorn torch clip openai numpy

# 启动服务
python rag_service.py
```
*服务将在 `http://0.0.0.0:8001` 监听。*

#### 步骤 3: 启动前端应用
```bash
# 进入项目目录
cd /mnt/bit/liyuanxi/projects/zhongche/rag-chat-next

# 安装 Node 依赖
npm install

# 启动开发服务器
npm run dev
```
*应用将在 `http://localhost:3000` 启动。*

---

## 6. 关键代码片段解析

### 6.1 强制格式输出 Prompt (`app/api/chat/route.ts`)
为了保证维修指南的规范性，我们在 Prompt 中加入了严格的指令：

```typescript
const promptContent = referenceDoc
    ? `${userText} Please answer based on the reference document...
请严格按照以下格式回答：
以下是需要的工具。
工具：{工具1}、{工具2}、{工具3}……
步骤：
- 第1步：{步骤1}
- 第2步：{步骤2}
......

Reference Document: ${referenceDoc}`
    : userText
```

### 6.2 向量相似度计算 (`rag_service.py`)
使用 PyTorch 进行高效的矩阵运算：

```python
# 文本编码
text_embed = model.encode_text(text_token)
text_embed = text_embed / text_embed.norm(dim=-1, keepdim=True)

# 计算相似度 (Einstein Summation)
# jk: 查询向量维度, ijk: 数据库向量维度 -> ij: 相似度分数
similarities = torch.einsum("jk,ijk->ij", text_embed, embeddings_tensor)

# 获取最佳匹配
idx = torch.argmax(similarities, dim=0)
```

---

## 7. 总结
本项目通过前后端分离的架构，成功实现了一个高性能、易扩展的设备维修智能问答系统。前端专注于提供流畅的交互体验和多任务管理，后端专注于复杂的语义检索和逻辑生成，两者通过标准的 RESTful API 结合，构成了一个完整的 RAG 应用闭环。
